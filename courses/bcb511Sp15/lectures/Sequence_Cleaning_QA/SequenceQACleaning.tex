\documentclass[pdf]{beamer}
\usepackage[latin1]{inputenc}
\usepackage{multirow}
\usetheme{Warsaw} %Warsaw
\usecolortheme{seahorse}


\begin{document}

\title[Sequence Mapping]{Sequence QA and Cleaning}
\subtitle{BCB 504: Applied Bioinformatics\\}
\author[Matt Settles]{Matt Settles}
\institute{University of Idaho\\ Bioinformatics and Computational Biology Program}
\date{\today}


%% Title page
\begin{frame}[plain]
  \titlepage
\end{frame}


%% Outline
\begin{frame}[plain] 
  \frametitle{Outline}
  \tableofcontents
\end{frame}

\section{QA}
\section{QA on raw data}
\begin{frame}
\frametitle{QA}
Its best to perform QA on both the run as a whole (poor samples can affect other samples) and on the samples themselves. Reports such as Basespace for Illumina,  are great ways to evaluate the runs as a whole.\\
\vspace{0.2in}
QA on the sample data can occur using 3rd party applications that evaluate quality.\\
Such as the fastqc application.\\
\end{frame}


\section{Cleaning reads}

\begin{frame}
\frametitle{Why Clean Reads}
We have found that aggressively "cleaning" and processing reads make a large difference to speed and quality of assembly and mapping results.
\vspace{0.2in}
Cleaning your reads means, removing reads/bases that are:
\begin{itemize}
\item not of primary interest (contamination)
\item originate from PCR duplication
\item artificially added onto sequence of primary interest (vectors, adapters, primers)
\item low quality bases
\item other unwanted sequence (polyA tails in RNA-seq data)
\item join short overlapping paired-end reads
\end{itemize}

\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Cleaning reads, some strategies}
\begin{itemize}
\item \textbf{Quality trim/cut}
\begin{itemize} 
\item 'end' trim a read until the average quality $>$ Q (Lucy)
\item remove any read with average quality $<$ Q
\end{itemize}
\item \textbf{eliminate singletons}
\begin{itemize}
\item If you have excess depth of coverage, and particularly if you have at least x-fold coverage where x is the read length, then eliminating singletons is a nice way of dramatically reducing the number of error-prone reads.
\end{itemize}
\item \textbf{eliminate all reads (pairs) containing an N}
\begin{itemize}
\item If you can afford the loss of coverage, you might throw away all reads containing Ns.
\end{itemize}
\item \textbf{Identity and trim off adapter and barcodes if present}
\begin{itemize}
\item Believe it or not, the software provided by Roche or Illumina, either does not look for, or does a mediocre job of, identifying adapter and removing them.
\end{itemize}
\item \textbf{Identity and remove contaminant and vector reads}
\begin{itemize}
\item Reads which appear to fully come from extraneous sequence should be removed.
\end{itemize}

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{preproc\_experiment}
preproc\_experiment is a command line pipeline (Written in R) that employs most of the techniques described above. It can be found on the IBEST CRC servers in the grc/2.0 module.
\end{frame}

\begin{frame}
\frametitle{Aggressive cleaning and filtering of raw reads}
\alert{Its better to have low coverage of really good quality sequence data, than high coverage of poor quality data}
\vspace{0.5cm}
\begin{enumerate}
\item Remove PCR duplicates (we use bases 10-35 of each read)
\item Remove/Trim Contaminants (at least PhiX) and Vectors (if used)
\item Search for and remove Illumina adapters
\item Trim sequences (Left and Right) by quality score (I like Q24)
\item If RNA and if mapping, trim polyA/T
\item Join and extend, overlapping paired end reads
\item Filter out rRNA genes (if ribosomal depletion was performed)
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Cleaning as QA}
Beyond generating better data for downstream analysis, cleaning statistics also give you an idea as to the quality of the sample, library generation, and sequencing technique used to generate the data. It can help inform you of what you might do in the future.
\end{frame}



\end{document}
